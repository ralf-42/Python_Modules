#@title üõ†Ô∏è Code M14_Modul { display-mode: "form" }
"""
Multimodales RAG Modul mit Bildbeschreibungen (Version 3)

Verwendung:
    # System initialisieren
    rag = init_rag_system()

    # Verzeichnis verarbeiten
    process_directory(rag, './files', auto_describe_images=True)

    # Suchen
    result = multimodal_search(rag, "Roboter")

    # NEU: Bild ‚Üí Bild Suche
    similar_images = search_similar_images(rag, "./query_image.jpg", k=5)

    # NEU: Bild ‚Üí Text Suche
    text_results = search_text_by_image(rag, "./query_image.jpg", k=3)

Autor: Enhanced by Claude
Datum: Oktober 2025
Version: 3.0 - Neue Features: Bild ‚Üí Bild Suche, Bild ‚Üí Text Suche
"""

from pathlib import Path
import uuid
import base64
import shutil
from dataclasses import dataclass

from markitdown import MarkItDown
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from sentence_transformers import SentenceTransformer
from PIL import Image
import chromadb


# ============================================================================
# KONFIGURATION
# ============================================================================

@dataclass
class RAGConfig:
    """Zentrale Konfiguration f√ºr das RAG-System"""
    chunk_size: int = 200
    chunk_overlap: int = 20
    text_threshold: float = 1.2
    image_threshold: float = 0.8
    clip_model: str = 'clip-ViT-B-32'
    text_model: str = 'text-embedding-3-small'
    llm_model: str = 'gpt-4o-mini'
    vision_model: str = 'gpt-4o-mini'
    db_path: str = './multimodal_rag_db'


@dataclass
class RAGComponents:
    """Container f√ºr alle RAG-System-Komponenten"""
    text_embeddings: OpenAIEmbeddings
    clip_model: SentenceTransformer
    llm: ChatOpenAI
    vision_llm: ChatOpenAI
    text_splitter: RecursiveCharacterTextSplitter
    markitdown: MarkItDown
    chroma_client: chromadb.PersistentClient
    text_collection: Chroma
    image_collection: any
    config: RAGConfig


# ============================================================================
# SYSTEM-INITIALISIERUNG
# ============================================================================

def init_rag_system(config=None):
    """
    Initialisiert das vollst√§ndige RAG-System mit Vision-LLM

    Args:
        config: Optional - RAGConfig Instanz

    Returns:
        RAGComponents mit allen Komponenten
    """
    if config is None:
        config = RAGConfig()

    print(f"üöÄ Initialisiere Enhanced RAG-System in {config.db_path}")

    # KI-Modelle laden
    text_embeddings = OpenAIEmbeddings(model=config.text_model)
    print("‚úÖ OpenAI Text-Embeddings initialisiert")

    print("üñºÔ∏è Lade CLIP-Modell...")
    clip_model = SentenceTransformer(config.clip_model)
    print("‚úÖ CLIP-Modell geladen")

    llm = ChatOpenAI(model=config.llm_model, temperature=0)
    vision_llm = ChatOpenAI(model=config.vision_model, temperature=0)
    print("‚úÖ LLMs initialisiert (Text + Vision)")

    # Text-Verarbeitung
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap
    )
    markitdown = MarkItDown()

    # Datenbank einrichten
    Path(config.db_path).mkdir(exist_ok=True)
    chroma_client = chromadb.PersistentClient(path=config.db_path)

    # Text-Collection (f√ºr Text-Dokumente UND Bildbeschreibungen)
    text_collection = Chroma(
        collection_name="texts",
        embedding_function=text_embeddings,
        persist_directory=config.db_path
    )

    # Bild-Collection (f√ºr CLIP-Embeddings)
    collections = [c.name for c in chroma_client.list_collections()]
    if "images" in collections:
        image_collection = chroma_client.get_collection("images")
    else:
        image_collection = chroma_client.create_collection(
            name="images",
            metadata={"hnsw:space": "cosine"}
        )

    print("‚úÖ Collections initialisiert\n")

    return RAGComponents(
        text_embeddings, clip_model, llm, vision_llm, text_splitter,
        markitdown, chroma_client, text_collection, image_collection, config
    )


# ============================================================================
# BILDBESCHREIBUNGS-GENERIERUNG
# ============================================================================

def generate_image_description(vision_llm, image_path):
    """
    Generiert eine detaillierte Beschreibung eines Bildes mit GPT-4o-mini

    Args:
        vision_llm: ChatOpenAI Instanz mit Vision-Unterst√ºtzung
        image_path: Pfad zum Bild

    Returns:
        String mit Bildbeschreibung oder Fallback bei Fehler
    """
    try:
        # Bild laden und in Base64 konvertieren
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')

        # Dateiendung ermitteln
        image_extension = Path(image_path).suffix.lower().replace('.', '')
        if image_extension == 'jpg':
            image_extension = 'jpeg'

        # Prompt f√ºr detaillierte Bildbeschreibung
        prompt = [
            {
                "type": "text",
                "text": """Analysiere dieses Bild detailliert und erstelle eine pr√§zise Beschreibung auf Deutsch.

Beschreibe:
1. Hauptobjekte und -personen
2. Farben und Stimmung
3. Komposition und Setting
4. Besondere Details oder Merkmale
5. M√∂glichen Kontext oder Zweck

Halte die Beschreibung pr√§gnant aber informativ (2-4 S√§tze)."""
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/{image_extension};base64,{image_data}"
                }
            }
        ]

        # Beschreibung generieren
        response = vision_llm.invoke([{"role": "user", "content": prompt}])
        description = response.content.strip()

        print(f"üìù Bildbeschreibung generiert: {description[:100]}...")
        return description

    except Exception as e:
        print(f"‚ùå Fehler bei Bildbeschreibung f√ºr {Path(image_path).name}: {e}")
        # Fallback: Dateiname als Beschreibung
        return Path(image_path).stem.replace('_', ' ').replace('-', ' ')


# ============================================================================
# DOKUMENT-VERARBEITUNG
# ============================================================================

def add_text_document(components, file_path):
    """
    F√ºgt ein Text-Dokument zur Datenbank hinzu

    Args:
        components: RAG-System-Komponenten
        file_path: Pfad zum Dokument

    Returns:
        bool - Erfolg
    """
    path = Path(file_path).absolute()

    # Duplikatspr√ºfung
    if components.text_collection.get(where={"source": str(path)})['ids']:
        print(f"‚ö†Ô∏è {path.name} bereits vorhanden")
        return False

    try:
        # Dokument mit MarkItDown konvertieren
        result = components.markitdown.convert(str(path))
        if not result or not result.text_content.strip():
            print(f"‚ö†Ô∏è {path.name} enth√§lt keinen Text")
            return False

        # Text in Chunks aufteilen
        chunks = components.text_splitter.split_text(result.text_content)
        documents = [
            Document(
                page_content=chunk.strip(),
                metadata={
                    "source": str(path),
                    "filename": path.name,
                    "chunk_id": i,
                    "doc_type": "text_document"
                }
            ) for i, chunk in enumerate(chunks) if chunk.strip()
        ]

        # Zur Datenbank hinzuf√ºgen
        if documents:
            components.text_collection.add_documents(documents)
            print(f"‚úÖ {len(documents)} Chunks von '{path.name}' hinzugef√ºgt")
            return True

    except Exception as e:
        print(f"‚ùå Fehler bei {path.name}: {e}")

    return False


def add_image_with_description(components, image_path, auto_describe=True):
    """
    F√ºgt ein Bild mit automatischer Beschreibung zur Datenbank hinzu

    Args:
        components: RAG-System-Komponenten
        image_path: Pfad zum Bild
        auto_describe: Automatische Beschreibung mit GPT-4o-mini

    Returns:
        Tuple (success: bool, text_doc_id: str oder None)
    """
    path = Path(image_path).absolute()

    if not path.exists():
        print(f"‚ùå Bild nicht gefunden: {path}")
        return False, None

    # Duplikatspr√ºfung
    if components.image_collection.get(where={"source": str(path)})['ids']:
        print(f"‚ö†Ô∏è Bild bereits vorhanden: {path.name}")
        return False, None

    try:
        # Bild laden und CLIP-Embedding erstellen
        image = Image.open(path).convert('RGB')
        clip_embedding = components.clip_model.encode(image).tolist()
        print(f"üñºÔ∏è Bild-Embedding erstellt f√ºr {path.name}")

        # Bildbeschreibung generieren
        description = ""
        if auto_describe:
            description = generate_image_description(components.vision_llm, str(path))
        else:
            description = path.stem.replace('_', ' ').replace('-', ' ')

        # IDs generieren
        image_doc_id = f"img_{uuid.uuid4().hex[:8]}_{path.name}"
        text_doc_id = f"img_desc_{uuid.uuid4().hex[:8]}_{path.stem}"

        # 1. Bildbeschreibung in Text-Collection speichern
        text_document = Document(
            page_content=f"Bildbeschreibung f√ºr {path.name}: {description}",
            metadata={
                "source": str(path),
                "filename": path.name,
                "doc_type": "image_description",
                "image_doc_id": image_doc_id,
                "description": description,
                "has_clip_embedding": True
            }
        )
        components.text_collection.add_documents([text_document], ids=[text_doc_id])
        print(f"‚úÖ Bildbeschreibung in Text-Collection gespeichert")

        # 2. Bild in Bild-Collection speichern mit Cross-Reference
        components.image_collection.add(
            ids=[image_doc_id],
            embeddings=[clip_embedding],
            documents=[f"Bild: {path.name} - {description}"[:1000]],
            metadatas=[{
                "source": str(path),
                "filename": path.name,
                "description": description[:500],
                "text_doc_id": text_doc_id
            }]
        )

        print(f"‚úÖ Bild '{path.name}' mit Cross-References hinzugef√ºgt\n")
        return True, text_doc_id

    except Exception as e:
        print(f"‚ùå Fehler bei Bild {path.name}: {e}")
        return False, None


def process_directory(components, directory, include_images=True, auto_describe_images=True):
    """
    Verarbeitet alle Dateien in einem Verzeichnis

    Args:
        components: RAG-System-Komponenten
        directory: Verzeichnispfad
        include_images: Bilder verarbeiten
        auto_describe_images: Automatische Bildbeschreibungen

    Returns:
        Dictionary mit Statistiken
    """
    dir_path = Path(directory)
    if not dir_path.exists():
        print(f"‚ùå Verzeichnis nicht gefunden: {directory}")
        return {"texts": 0, "images": 0, "image_descriptions": 0}

    # Unterst√ºtzte Dateitypen
    text_extensions = {'.pdf', '.docx', '.txt', '.md', '.html'}
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}

    # Dateien sammeln
    text_files = [f for f in dir_path.rglob("*") if f.suffix.lower() in text_extensions]
    image_files = [f for f in dir_path.rglob("*") if f.suffix.lower() in image_extensions] if include_images else []

    print(f"üìä Gefunden: {len(text_files)} Text-Dateien, {len(image_files)} Bilder\n")

    # Text-Dateien verarbeiten
    text_count = 0
    for file_path in text_files:
        print(f"üìÑ {file_path.name}")
        if add_text_document(components, str(file_path)):
            text_count += 1

    print()  # Leerzeile

    # Bild-Dateien verarbeiten
    image_count = 0
    image_desc_count = 0
    for img_path in image_files:
        print(f"üñºÔ∏è {img_path.name}")
        success, text_doc_id = add_image_with_description(
            components,
            str(img_path),
            auto_describe=auto_describe_images
        )
        if success:
            image_count += 1
            if text_doc_id:
                image_desc_count += 1

    return {
        "texts": text_count,
        "images": image_count,
        "image_descriptions": image_desc_count
    }


# ============================================================================
# SUCHFUNKTIONEN
# ============================================================================

def search_texts(components, query, k=3, include_image_descriptions=True):
    """
    Durchsucht Text-Dokumente inkl. Bildbeschreibungen

    Args:
        components: RAG-System-Komponenten
        query: Suchanfrage
        k: Anzahl Ergebnisse
        include_image_descriptions: Bildbeschreibungen einschlie√üen

    Returns:
        Formatierter String mit Ergebnissen
    """
    if not components.text_collection.get()['ids']:
        return "‚ùå Keine Text-Dokumente gefunden"

    # √Ñhnlichkeitssuche durchf√ºhren
    docs_with_scores = components.text_collection.similarity_search_with_score(query, k=k*2)
    if not docs_with_scores:
        return "‚ùå Keine relevanten Dokumente gefunden"

    # FIX 1: Score in √Ñhnlichkeit umwandeln (ChromaDB liefert L2-Distanz: 0=identisch, 2=maximal entfernt)
    docs_with_similarity = []
    for doc, score in docs_with_scores:
        similarity = max(0, 1 - (score / 2))  # Konvertiere Distanz zu √Ñhnlichkeit
        docs_with_similarity.append((doc, similarity))

    # Nach √Ñhnlichkeit sortieren
    docs_with_similarity.sort(key=lambda x: x[1], reverse=True)

    # Filtern nach Mindest-√Ñhnlichkeit
    min_similarity = 0.3
    relevant_docs = [(doc, sim) for doc, sim in docs_with_similarity[:k]
                     if sim >= min_similarity]

    if not relevant_docs:
        return "‚ùå Keine ausreichend √§hnlichen Dokumente gefunden"

    # Dokumente nach Typ trennen
    text_docs = []
    image_desc_docs = []

    for doc, sim in relevant_docs:
        doc_type = doc.metadata.get("doc_type", "text_document")
        if doc_type == "image_description" and include_image_descriptions:
            image_desc_docs.append((doc, sim))
        elif doc_type == "text_document":
            text_docs.append((doc, sim))

    # Kontext f√ºr LLM zusammenstellen
    all_docs = text_docs + image_desc_docs
    context = "\n\n---\n\n".join([doc.page_content for doc, _ in all_docs])

    # Quellen sammeln
    sources = [
        {
            "filename": doc.metadata.get("filename", "Unbekannt"),
            "similarity": round(sim, 3),
            "type": doc.metadata.get("doc_type", "text_document"),
            "image_doc_id": doc.metadata.get("image_doc_id")
        }
        for doc, sim in all_docs
    ]

    # LLM-Antwort generieren
    prompt = f"""Beantworte die Frage pr√§zise basierend auf dem Kontext.

KONTEXT:
{context}

FRAGE: {query}

ANTWORT:"""

    response = components.llm.invoke(prompt).content

    # Ausgabe mit separaten Quellenlisten
    text_sources = [s for s in sources if s['type'] == 'text_document']
    image_sources = [s for s in sources if s['type'] == 'image_description']

    result = response

    if text_sources:
        result += f"\n\nüìö Text-Quellen ({len(text_sources)}): " + "\n".join([
            f"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})"
            for src in text_sources
        ])

    if image_sources:
        result += f"\n\nüñºÔ∏è Relevante Bilder ({len(image_sources)}): " + "\n".join([
            f"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})"
            for src in image_sources
        ])

    return result


def search_images(components, query, k=3):
    """
    Durchsucht Bilder mit Text-Query √ºber CLIP

    Args:
        components: RAG-System-Komponenten
        query: Suchanfrage
        k: Anzahl Ergebnisse

    Returns:
        Liste von Bildern mit Metadaten
    """
    if components.image_collection.count() == 0:
        return []

    # Text-Query in Bild-Embedding-Raum umwandeln
    query_embedding = components.clip_model.encode(query).tolist()

    # Suche in Bild-Collection
    results = components.image_collection.query(
        query_embeddings=[query_embedding],
        n_results=min(k*2, components.image_collection.count()),
        include=['documents', 'metadatas', 'distances']
    )

    if not results['ids'][0]:
        return []

    # Ergebnisse filtern und formatieren
    return [
        {
            "filename": metadata.get("filename", "Unbekannt"),
            "path": metadata.get("source", ""),
            "description": metadata.get("description", ""),
            "text_doc_id": metadata.get("text_doc_id", ""),
            "similarity": round(max(0, 1 - distance), 3)
        }
        for distance, metadata in zip(results['distances'][0], results['metadatas'][0])
        if distance < components.config.image_threshold
    ]


def find_related_images_from_text(components, text_doc_ids, k=3):
    """
    Findet Bilder √ºber ihre Textbeschreibungen (Cross-Modal-Retrieval)

    Args:
        components: RAG-System-Komponenten
        text_doc_ids: Liste von Text-Dokument-IDs
        k: Maximale Anzahl Bilder

    Returns:
        Liste von verwandten Bildern
    """
    related_images = []

    for text_id in text_doc_ids:
        try:
            doc_data = components.text_collection.get(ids=[text_id])
            if not doc_data['ids']:
                continue

            metadata = doc_data['metadatas'][0]

            # Pr√ºfe ob es eine Bildbeschreibung ist
            if metadata.get('doc_type') == 'image_description':
                image_doc_id = metadata.get('image_doc_id')

                if image_doc_id:
                    # Hole das zugeh√∂rige Bild
                    image_data = components.image_collection.get(ids=[image_doc_id])

                    if image_data['ids']:
                        img_metadata = image_data['metadatas'][0]
                        related_images.append({
                            'filename': img_metadata.get('filename', 'Unbekannt'),
                            'path': img_metadata.get('source', ''),
                            'description': img_metadata.get('description', ''),
                            'source': 'cross_modal_retrieval'
                        })
        except Exception as e:
            print(f"‚ö†Ô∏è Fehler beim Cross-Modal-Retrieval: {e}")
            continue

    return related_images[:k]


def search_similar_images(components, query_image_path, k=5):
    """
    Bild ‚Üí Bild Suche: Findet visuell √§hnliche Bilder in der Datenbank

    Args:
        components: RAG-System-Komponenten
        query_image_path: Pfad zum Query-Bild
        k: Anzahl √§hnlicher Bilder

    Returns:
        Liste von √§hnlichen Bildern mit Metadaten und √Ñhnlichkeitswerten
    """
    path = Path(query_image_path)

    if not path.exists():
        print(f"‚ùå Query-Bild nicht gefunden: {query_image_path}")
        return []

    if components.image_collection.count() == 0:
        print("‚ùå Keine Bilder in der Datenbank")
        return []

    try:
        # Query-Bild laden und CLIP-Embedding erstellen
        image = Image.open(path).convert('RGB')
        query_embedding = components.clip_model.encode(image).tolist()
        print(f"üîç CLIP-Embedding erstellt f√ºr Query-Bild: {path.name}")

        # Suche in Bild-Collection
        results = components.image_collection.query(
            query_embeddings=[query_embedding],
            n_results=min(k, components.image_collection.count()),
            include=['documents', 'metadatas', 'distances']
        )

        if not results['ids'][0]:
            return []

        # Ergebnisse formatieren
        similar_images = [
            {
                "filename": metadata.get("filename", "Unbekannt"),
                "path": metadata.get("source", ""),
                "description": metadata.get("description", ""),
                "similarity": round(max(0, 1 - distance), 3)
            }
            for distance, metadata in zip(results['distances'][0], results['metadatas'][0])
        ]

        print(f"‚úÖ {len(similar_images)} √§hnliche Bilder gefunden\n")
        return similar_images

    except Exception as e:
        print(f"‚ùå Fehler bei Bild-zu-Bild-Suche: {e}")
        return []


def search_text_by_image(components, query_image_path, k=3, k_text=3):
    """
    Bild ‚Üí Text Suche: Findet √§hnliche Bilder und relevante Text-Dokumente

    Kombiniert:
    1. Visuelle √Ñhnlichkeitssuche (Bild ‚Üí Bild via CLIP)
    2. Semantische Textsuche basierend auf Bildbeschreibungen

    Args:
        components: RAG-System-Komponenten
        query_image_path: Pfad zum Query-Bild
        k: Anzahl √§hnlicher Bilder
        k_text: Anzahl relevanter Text-Dokumente

    Returns:
        Formatierter String mit Bildern und Text-Dokumenten
    """
    path = Path(query_image_path)

    if not path.exists():
        return f"‚ùå Query-Bild nicht gefunden: {query_image_path}"

    print(f"\n{'='*70}")
    print(f"üîç Bild ‚Üí Text Suche f√ºr: {path.name}")
    print(f"{'='*70}\n")

    # 1. √Ñhnliche Bilder finden
    similar_images = search_similar_images(components, query_image_path, k)

    if not similar_images:
        return "‚ùå Keine √§hnlichen Bilder gefunden"

    # 2. Bildbeschreibungen aus text_collection holen
    image_descriptions = []
    for img in similar_images:
        img_path = img.get('path', '')
        if img_path:
            try:
                text_docs = components.text_collection.get(
                    where={
                        "$and": [
                            {"source": img_path},
                            {"doc_type": "image_description"}
                        ]
                    }
                )
                if text_docs['ids']:
                    image_descriptions.append({
                        'content': text_docs['documents'][0],
                        'metadata': text_docs['metadatas'][0],
                        'similarity': img['similarity']
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è Fehler beim Abrufen der Beschreibung f√ºr {img.get('filename', 'Unbekannt')}: {e}")
                continue

    if not image_descriptions:
        return "‚ùå Keine Textbeschreibungen f√ºr √§hnliche Bilder gefunden"

    # 3. Semantische Textsuche basierend auf Bildbeschreibungen
    # Erstelle Suchquery aus den Bildbeschreibungen
    search_query = " ".join([desc['content'] for desc in image_descriptions[:2]])

    print(f"üîç Suche nach relevanten Text-Dokumenten basierend auf Bildbeschreibungen...")

    # Suche Text-Dokumente (nicht Bildbeschreibungen)
    try:
        docs_with_scores = components.text_collection.similarity_search_with_score(
            search_query,
            k=k_text * 2
        )

        # Filtere nur echte Text-Dokumente (keine Bildbeschreibungen)
        text_documents = []
        for doc, score in docs_with_scores:
            if doc.metadata.get('doc_type') == 'text_document':
                similarity = max(0, 1 - (score / 2))
                if similarity >= 0.3:  # Mindest-√Ñhnlichkeit
                    text_documents.append({
                        'content': doc.page_content,
                        'filename': doc.metadata.get('filename', 'Unbekannt'),
                        'similarity': round(similarity, 3)
                    })
                    if len(text_documents) >= k_text:
                        break

        print(f"‚úÖ {len(text_documents)} relevante Text-Dokumente gefunden\n")

    except Exception as e:
        print(f"‚ö†Ô∏è Fehler bei Textsuche: {e}")
        text_documents = []

    # 4. LLM-Zusammenfassung generieren
    try:
        # Kontext f√ºr Bilder
        image_context_parts = []
        for desc in image_descriptions:
            image_context_parts.append(
                f"Bild: {desc['metadata'].get('filename', 'Unbekannt')} "
                f"(√Ñhnlichkeit: {desc['similarity']})\n{desc['content']}"
            )

        image_context = "\n\n---\n\n".join(image_context_parts)

        # Kontext f√ºr Text-Dokumente
        text_context = ""
        if text_documents:
            text_context_parts = [
                f"Dokument: {doc['filename']} (√Ñhnlichkeit: {doc['similarity']})\n{doc['content'][:500]}"
                for doc in text_documents
            ]
            text_context = "\n\n---\n\n".join(text_context_parts)

        # Prompt f√ºr LLM
        prompt = f"""Du hast ein Query-Bild erhalten. Basierend auf visueller √Ñhnlichkeit und semantischer Suche wurden folgende Informationen gefunden.

VISUELL √ÑHNLICHE BILDER UND IHRE BESCHREIBUNGEN:
{image_context}

{'RELEVANTE TEXT-DOKUMENTE:' if text_documents else '(Keine relevanten Text-Dokumente gefunden)'}
{text_context if text_documents else ''}

AUFGABE:
Erstelle eine pr√§gnante Zusammenfassung der wichtigsten Informationen, die zum Query-Bild passen.
Beschreibe Gemeinsamkeiten, wichtige Merkmale und relevante Kontextinformationen aus den Texten."""

        response = components.llm.invoke(prompt).content

        # Formatierte Ausgabe
        result = f"üìÑ ZUSAMMENFASSUNG (Bild ‚Üí Text):\n{'-'*70}\n{response}\n\n"

        # Bild-Quellen
        result += f"üñºÔ∏è GEFUNDENE √ÑHNLICHE BILDER ({len(similar_images)}):\n{'-'*70}\n"
        for i, img in enumerate(similar_images, 1):
            result += f"   {i}. {img['filename']} (√Ñhnlichkeit: {img['similarity']})\n"
            if img['description']:
                result += f"      üìù {img['description'][:300]}...\n"

        # Text-Quellen
        if text_documents:
            result += f"\nüìö RELEVANTE TEXT-DOKUMENTE ({len(text_documents)}):\n{'-'*70}\n"
            for i, doc in enumerate(text_documents, 1):
                result += f"   {i}. {doc['filename']} (√Ñhnlichkeit: {doc['similarity']})\n"
                result += f"      üìÑ {doc['content'][:200]}...\n"
        else:
            result += f"\nüìö Keine relevanten Text-Dokumente gefunden.\n"

        return result

    except Exception as e:
        print(f"‚ùå Fehler bei Textgenerierung: {e}")
        return f"‚ùå Fehler bei der Verarbeitung: {e}"


def multimodal_search(components, query, k_text=3, k_images=3, enable_cross_modal=True):
    """
    F√ºhrt erweiterte multimodale Suche durch

    Kombiniert:
    - Text-Suche (inkl. Bildbeschreibungen)
    - CLIP-basierte Bildsuche
    - Cross-Modal-Retrieval (Text ‚Üí Bild √ºber Beschreibungen)

    Args:
        components: RAG-System-Komponenten
        query: Suchanfrage
        k_text: Anzahl Text-Ergebnisse
        k_images: Anzahl Bild-Ergebnisse
        enable_cross_modal: Cross-Modal-Retrieval aktivieren

    Returns:
        Formatierter String mit allen Ergebnissen
    """
    print(f"\n{'='*70}")
    print(f"üîç Multimodale Suche: {query}")
    print(f"{'='*70}\n")

    # FIX 3: Text-Suche nur EINMAL durchf√ºhren und Ergebnisse wiederverwenden
    docs_with_scores = components.text_collection.similarity_search_with_score(query, k=k_text*2)

    # Konvertiere Scores zu √Ñhnlichkeiten
    docs_with_similarity = []
    for doc, score in docs_with_scores:
        similarity = max(0, 1 - (score / 2))
        docs_with_similarity.append((doc, similarity))

    docs_with_similarity.sort(key=lambda x: x[1], reverse=True)

    # Filtern nach Mindest-√Ñhnlichkeit
    min_similarity = 0.3
    relevant_docs = [(doc, sim) for doc, sim in docs_with_similarity[:k_text]
                     if sim >= min_similarity]

    # 1. Text-Ergebnisse formatieren (ohne nochmalige Suche)
    if not relevant_docs:
        text_results = "‚ùå Keine ausreichend √§hnlichen Dokumente gefunden"
    else:
        # Dokumente nach Typ trennen
        text_docs = []
        image_desc_docs = []

        for doc, sim in relevant_docs:
            doc_type = doc.metadata.get("doc_type", "text_document")
            if doc_type == "image_description":
                image_desc_docs.append((doc, sim))
            elif doc_type == "text_document":
                text_docs.append((doc, sim))

        # Kontext f√ºr LLM zusammenstellen
        all_docs = text_docs + image_desc_docs
        context = "\n\n---\n\n".join([doc.page_content for doc, _ in all_docs])

        # Quellen sammeln
        sources = [
            {
                "filename": doc.metadata.get("filename", "Unbekannt"),
                "similarity": round(sim, 3),
                "type": doc.metadata.get("doc_type", "text_document"),
                "image_doc_id": doc.metadata.get("image_doc_id")
            }
            for doc, sim in all_docs
        ]

        # LLM-Antwort generieren
        prompt = f"""Beantworte die Frage pr√§zise basierend auf dem Kontext.

KONTEXT:
{context}

FRAGE: {query}

ANTWORT:"""

        response = components.llm.invoke(prompt).content

        # Ausgabe mit separaten Quellenlisten
        text_sources = [s for s in sources if s['type'] == 'text_document']
        image_sources = [s for s in sources if s['type'] == 'image_description']

        text_results = response

        if text_sources:
            text_results += f"\n\nüìö Text-Quellen ({len(text_sources)}): " + "\n".join([
                f"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})"
                for src in text_sources
            ])

        if image_sources:
            text_results += f"\n\nüñºÔ∏è Relevante Bilder ({len(image_sources)}): " + "\n".join([
                f"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})"
                for src in image_sources
            ])

    # 2. Direkte Bild-Suche √ºber CLIP
    image_results = search_images(components, query, k_images)

    # 3. Cross-Modal-Retrieval (FIX 2: N+1 Query Problem behoben)
    cross_modal_images = []
    if enable_cross_modal and docs_with_scores:
        # FIX 2: IDs direkt aus docs_with_scores extrahieren statt zus√§tzliche get() Calls
        image_desc_docs_for_retrieval = [
            (doc, sim) for doc, sim in docs_with_similarity
            if doc.metadata.get('doc_type') == 'image_description'
        ]

        if image_desc_docs_for_retrieval:
            # Extrahiere image_doc_ids direkt aus den Metadaten
            image_doc_ids = [
                doc.metadata.get('image_doc_id')
                for doc, _ in image_desc_docs_for_retrieval
                if doc.metadata.get('image_doc_id')
            ]

            # Hole Bilder f√ºr diese IDs (ein einziger Batch-Call statt N einzelne)
            if image_doc_ids:
                try:
                    image_data = components.image_collection.get(ids=image_doc_ids)

                    for img_id, img_metadata in zip(image_data['ids'], image_data['metadatas']):
                        cross_modal_images.append({
                            'filename': img_metadata.get('filename', 'Unbekannt'),
                            'path': img_metadata.get('source', ''),
                            'description': img_metadata.get('description', ''),
                            'source': 'cross_modal_retrieval'
                        })
                except Exception as e:
                    print(f"‚ö†Ô∏è Fehler beim Cross-Modal-Retrieval: {e}")

            # Limitiere auf k_images
            cross_modal_images = cross_modal_images[:k_images]

    # Ergebnisse zusammenfassen
    result = f"üìÑ TEXT-ERGEBNISSE:\n{'-'*70}\n{text_results}\n\n"

    if image_results:
        result += f"üñºÔ∏è BILD-ERGEBNISSE via CLIP ({len(image_results)} gefunden):\n{'-'*70}\n"
        for i, img in enumerate(image_results, 1):
            result += f"   {i}. {img['filename']} (√Ñhnlichkeit: {img['similarity']})\n"
            if img['description']:
                result += f"      üìù {img['description'][:600]}...\n"
    else:
        result += f"üñºÔ∏è Keine relevanten Bilder via CLIP gefunden.\n"

    if cross_modal_images:
        result += f"\nüîó CROSS-MODAL RETRIEVAL ({len(cross_modal_images)} Bilder via Textsuche):\n{'-'*70}\n"
        for i, img in enumerate(cross_modal_images, 1):
            result += f"   {i}. {img['filename']}\n"
            if img['description']:
                result += f"      üìù {img['description'][:600]}...\n"

    return result


# ============================================================================
# HILFSFUNKTIONEN
# ============================================================================

def get_system_status(components):
    """Gibt System-Status zur√ºck"""
    text_data = components.text_collection.get()
    all_text_count = len(text_data['ids'])

    text_docs_count = sum(1 for meta in text_data['metadatas']
                         if meta.get('doc_type') == 'text_document')
    image_desc_count = sum(1 for meta in text_data['metadatas']
                          if meta.get('doc_type') == 'image_description')

    image_count = components.image_collection.count()

    return {
        "text_chunks": text_docs_count,
        "image_descriptions": image_desc_count,
        "images": image_count,
        "total_text_entries": all_text_count,
        "total_documents": all_text_count + image_count
    }


def cleanup_database(db_path='./multimodal_rag_db'):
    """L√∂scht die Datenbank komplett"""
    if Path(db_path).exists():
        shutil.rmtree(db_path)
        print(f"üóëÔ∏è Datenbank gel√∂scht: {db_path}")